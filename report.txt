CS 421 Natural Language Processing: Term Project Report

Team Information
- Hamza Mansoor - smans4@uic.edu
- Junha Liu - jliu238@uic.edu

Introduction
Our group was tasked with building an automatic essay scorer for non-native English speakers' essays. The goal was to develop a system capable of classifying essays into low or high scores based on various NLP techniques. This report summarizes our project's approach, the challenges we faced, our results, and potential improvements.

Approach and Implementation
Our project was structured into two main parts. In the first part, we focused on setting up the basic framework for the essay scorer, which involved defining the features for scoring, and developing initial scoring algorithms based on essay length and spelling errors. We utilized Python 3.10 and several NLP libraries for text processing and feature extraction.

In the second part of the project, we enhanced our scorer by incorporating advanced grammatical analysis using dependency parsing. We also integrated semantic analysis by employing word embeddings to gauge how well the essays addressed the given prompt and checked for coherence through simplistic reference resolution methods.

Challenges and Learning Outcomes

One of the significant challenges we encountered was the complexity of natural language, especially dealing with grammatical nuances in non-native English texts. While POS tagging helped identify some grammatical inconsistencies, dependency parsing was less effective due to the irregular syntactic structures in poorly written essays. This difficulty highlighted the limitations of current NLP tools in handling texts with high variability and errors.

Another challenge was ensuring the scorer's reliability across a diverse set of essays. Despite our efforts, the system sometimes misclassified essays, particularly those on the borderline between low and high scores. Through this project, we learned a great deal about the practical applications of NLP and the importance of feature selection process in developing an effective scoring model.

Results and Improvements
The automatic essay scorer performed reasonably well. To enhance our scorer's accuracy, we could integrate more sophisticated NLP models that better understand context and subtleties in language use, such as transformer-based models like BERT or GPT-3.5, which, though not allowed in this project for undergraduates, represent a potential direction for future iterations.

Incorporating a more comprehensive set of features, such as syntactic complexity and more advanced semantic analysis techniques, could also improve our system. Additionally, a more extensive training dataset could help the model learn more nuanced distinctions between high and low-quality essays.

Conclusion
This project provided us with invaluable hands-on experience in applying NLP techniques to solve real-world problems. We not only learned about the technical aspects of building an NLP application but also about the challenges involved in working with natural language data. Future improvements would involve adopting more advanced NLP techniques and expanding our dataset to refine our scoring algorithms further. This project has set a solid foundation for our future endeavors in the field of NLP.
